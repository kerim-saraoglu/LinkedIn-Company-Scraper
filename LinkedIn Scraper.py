import requests
from bs4 import BeautifulSoup
from selenium import webdriver
import pandas as pd
from time import sleep
from selenium.common.exceptions import NoSuchElementException

### The purpose of this code is to use LinkedIn to gather basic information about companies of interest.
### User inputs a CSV/Excel file consisting of links to company websites. Each website's source code
### is then searched for a LinkedIn company page URL. If one exists, the URL is scraped. The full list
### of URLs is then saved as a CSV file. The CSV file is referenced in the second portion of the code,
### with each LinkedIn URL being scraped for a company overview, industry, size, headquarters, and type.
### The final output is saved as a CSV file, which can be used as part of sourcing or outreach initiatives.

def getLinkedInURLs():
    
    # User must save a CSV/Excel file of company URLs in path. Function searches for LinkedIn badge on
    # website and, if one exists, stores URL of corresponding LinkedIn company page.
    
    url_sheet = pd.read_excel('path/filename.xlsx') # Reads sheet with company URLs.
    
    urls = url_sheet['URLs']
        
    linkedin_links = []
    urls_with_linkedin = []
    
    for i in range(0,len(urls)):
        url = urls.values[i]
        try:
            r = requests.get(url)
        except requests.exceptions.ConnectionError:
            r.status_code = "Connection refused"
        soup = BeautifulSoup(r.content, "html.parser")
        hrefs = soup.find_all("a", href=lambda href: href and "linkedin.com/company" in href)
        if len(hrefs) > 0:
            urls_with_linkedin.append(url)
        for href in hrefs:
            linkedin = href['href']
            linkedin_links.append(linkedin)
    
    linkedin_no_duplicates = []
    [linkedin_no_duplicates.append(x) for x in linkedin_links if x not in linkedin_no_duplicates]
    
    df = pd.DataFrame(linkedin_no_duplicates,columns=['URLs'])
    
    df.to_csv("LinkedIn_URLs.csv",index=False)


def scrapeLinkedIn():
    
    # Takes list of LinkedIn URLs generated by previous function as output. Scrapes each URL for basic
    # company information.
    
    linkedin_sheet = pd.read_csv("LinkedIn_URLs.csv")
    linkedin_urls = linkedin_sheet['URLs']
    
    driver = webdriver.Chrome()
    driver.get('https://www.linkedin.com/login?fromSignIn=true&trk=guest_homepage-basic_nav-header-signin')
    username = driver.find_element_by_name('session_key')
    username.send_keys('kerim_saraoglu@brown.edu')
    
    password = driver.find_element_by_name('session_password')
    password.send_keys('xxxxxxx') # Enter your password here.
    
    log_in_button = driver.find_element_by_xpath("//button[contains(@class, 'btn')]")
    log_in_button.click()
    
    all_company_info = []
    
    for link in linkedin_urls:
        driver.get(link)
        try:
            about_page = driver.find_element_by_xpath("//a[contains(@data-control-name, 'about_tab')]")
            about_page.click()
        except NoSuchElementException:
            continue
            
        sleep(2)
        
        company_info = []
            
        company_name = driver.find_element_by_xpath("//span[contains(@dir, 'ltr')]")
        company_info.append(company_name.text)
        
        try:
            overview = driver.find_element_by_xpath("//p[contains(@class, 'break-words')]")
            company_info.append(overview.text)
        except NoSuchElementException:
            overview = 'N/A'
            company_info.append(overview)
            
        details = driver.find_element_by_class_name("overflow-hidden")
        for row in details.find_elements_by_css_selector("dd"):
            company_info.append(row.text)
       
        sleep(2)
    
        all_company_info.append(company_info)
        
    df = pd.DataFrame(all_company_info)
    df.to_csv('All Company Information.csv',index=False)